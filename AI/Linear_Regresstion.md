### 비전인식
사람의 시야와 동일하게 구현

### **Rule based programming**
프로그래머가 사전에 룰을 알고 있음   
ex. 자동차를 자동차라고 판단할 수 있는 근거를 모르는 경우    
블랙박스 (왜 그런지 알 수 없는 것 - 내부가 감춰짐)     
내 프로그램이 무언가를 자동차로 판단하기 전까지 **자동차가 무엇인지 알려줌** → 이미지를 여러 개 줄 테니 너가 알아서 판단해라   

<br>

# 인공지능 교수법 (3가지)
방법론(머신러닝, 딥러닝 ..)       
교수법 (지도학습, 비지도학습, 강화학습)     

### 지도 학습
부모가 미리 결과를 알려주면서 학습     
프로그래머가 미리 **라벨링된 데이터**를 함께 주면서 학습       
지도학습의 한계 - 데이터 이상의 것을 배울 수 없다 = 부모를 뛰어 넘을 수 없음 = another case 확인 불가능        

### 비지도 학습
부모가 아무런 결과도 알려주지 않으면서 학습        
프로그래머가 **어떠한 라벨링된 데이터도 주지 않으면서** 학습

### 강화학습
부모가 **목표만 제시하되 어떠한 방법도 제시하지 않음**        
단, 목표를 **달성하면 보상**하고, 달성하지 **못하면 처벌**함          
-> 프로그래머는 프로그램에 달성해야 하는 목표(이를테면 총점을 높혀라)를 제시하고 프로그램이 어떤 알고리즘을 적용하는지는 상관하지 않되 목표를 달성하면 큰 점수를 부여하고(양수값의 점수) 목표를 달성하지 않으면 낮은 점수(이를테면 음수값의 점수)를 부여하여 **수백만번 같은 훈련**을 반복시킴

예시
- 지도학습 - 얼굴 감정 인식, 아이폰 첫 시리 세팅, 카메라 차량번호판 숫자 추출
- 비지도학습 - 연관상품 추천, 연관검색어, 유튜브 알고리즘, 휴대폰 얼굴인식(이미지 중에서 해당하는 부분을 뽑아내는 기술)
- 강화학습 - 주식투자

<br>

# 지도학습 - 선형 회귀
지도 학습: 라벨링된 기존 데이터를 학습시켜서 새로운 데이터가 왔을 때 결과 추론

### 회귀분석 Regression
함수
- x 하나에 y 하나가 대응 (인풋에 대한 결과값을 예측할 수 있어야 함 → 왜? 결과가 하나뿐일테니까)
- 입력과 출력이 있으면 함수

1차 함수
- 입력값(x)의 최고차수가 1차인 함수

### 선형회귀 (`Linear Regression`)
회귀선 - 데이터들과 오차의 합이 가장 적은 것이 적합한 것이 됨!           
기존 데이터들을 **가장 오차 없이 설명가능한 1차 함수의 식**을 놓은 후, 새로운 예측 데이터가 들어왔을 때 1차 함수를 바탕으로 예측하게 함 

*오차 = 비용 (cost) → 비용이 작은 함수가 목표! 
sigma n=1 to k (bn - f(an))^2


**Summary**
- h(x) = ax + b → 가설을 구하는 함수! (hypothesis)
    - **단서에 따라서 예측값이 정해지는** 함수
- cost 함수
    - 내가 세운 가설의 cost를 알게되는 함수
    - 예측 함수로부터 실데이터의 코스트

가설함수에 따라서 비용함수 값이 정해짐! 
h(x) = 3x+4 → 40 (cost 값)

cost 함수는 가설함수가 정해지면 **가설함수에 따른 비용 값**을 정해줌!!            
⇒ cost함수의 input값이 h(x)의 a, b ⇒ c(a, b)


### cost 함수
우리의 목표! : **cost 함수 값을 최소로 만드는 a와 b를 찾아야 함!**       

c(a, b) = c(w, b)       
h(x) = wx + b (w: weight, b: bias)      
c(w, b) = sigma k=1 to n (bk - h(ak))^2     
= sigma k=1 to n (bk - w*ak - b)^2

계산의 편의를 위해 h(x) = wx라 가정        
⇒ c(w) = sigma k=1 to n (bk - w*ak)^2 

즉, Linear Regerssion에서 h(x) = wx라 가정하면, **cost 함수는 w에 대한 2차 함수**임!      
2차 함수의 최대/최소 값을 구할 수 있듯이 **cost 함수의 최대/최소 값**을 구할 수 있다!         

최소 cost 값을 위해서 c'(w) = 0이 되는 w를 찾게 하면 됨     
c'(w) = 2(sigma k=1 to n ak^2)*w - 2(sigma k=1 to n ak*bk) = 0      
⇒ w = (sigma k=1 to n ak*bk) / (sigma k=1 to n ak^2)


# `Gradient Desent` - 경사하강법

### 여기서 의문!
토익 점수 영향 요소
- 토익공부 투입시간
- 나이
- 최종학력
- 유학기간
- 학원등록 여부
- 영단어 암기수

토익 점수에 영향끼치는 요소가 1가지만 있지는 않을 것. 분명 여러 개임         
→ 각 역향들이 모두 동일한 영향력을 갖는 것은 아님! 

h(x) = w_1x_1 + w_2x_2 + ... w_nx_n + b         
실제 가설함수는 변수가 다양할 것임 -> 각 변수의 **중요도(가중치: `weight`)**

cost(w1, w2, ..., wn, b)        
= sigma k=1 to n (bk - h(ak))^2
= sigma k=1 to n (bk - w1a1 - w2a2 - ... - wnan - b)^2

"편미분" - 미분하고자 하는 미지수 외에는 전부 상수 취급!      
i) w1에 대해 편미분 -> w1에 대한 1차식         
ii) w2에 대해 편미분 -> w2에 대한 1차식        
iii) w3에 대해 편미분 -> w3에 대한 1차식       
...     

=> 결국 식을 풀 수 없게 됨! 

### 경사하강법을 통해 해결해보자!
그래프의 기울기가 가파른 쪽으로 내려가다 보면 어느 순간 가장 낮은 값이 보이는데 그것을 최저값이라고 가정함.       
=> 어디서 시작하느냐에 따라 해당 구간에서 가장 낮은 값이 전 구간에서의 낮은 값임을 보장할 수 없음       
=> 경사하강법의 최대 단점!

### 해결?
그래프를 평평하게 펴야함! 어느 지점이 최저점인지 한번에 파악 할 수 있어야 함!       
현재 비용함수에 특정 공식을 곱하여 합성함수로 평평한 그래프를 만들면 됨! -> 경사하강법을 적용할 수 있는 극소값이 하나뿐이 그래프가 됨   





