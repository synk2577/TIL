### 비전인식
사람의 시야와 동일하게 구현

### **Rule based programming**
프로그래머가 사전에 룰을 알고 있음   
ex. 자동차를 자동차라고 판단할 수 있는 근거를 모르는 경우    
블랙박스 (왜 그런지 알 수 없는 것 - 내부가 감춰짐)     
내 프로그램이 무언가를 자동차로 판단하기 전까지 **자동차가 무엇인지 알려줌** → 이미지를 여러 개 줄 테니 너가 알아서 판단해라   

<br>

# 인공지능 교수법 (3가지)
방법론(머신러닝, 딥러닝 ..)       
교수법 (지도학습, 비지도학습, 강화학습)     

### 지도 학습
부모가 미리 결과를 알려주면서 학습     
프로그래머가 미리 **라벨링된 데이터**를 함께 주면서 학습       
지도학습의 한계 - 데이터 이상의 것을 배울 수 없다 = 부모를 뛰어 넘을 수 없음 = another case 확인 불가능        

### 비지도 학습
부모가 아무런 결과도 알려주지 않으면서 학습        
프로그래머가 **어떠한 라벨링된 데이터도 주지 않으면서** 학습

### 강화학습
부모가 **목표만 제시하되 어떠한 방법도 제시하지 않음**        
단, 목표를 **달성하면 보상**하고, 달성하지 **못하면 처벌**함          
-> 프로그래머는 프로그램에 달성해야 하는 목표(이를테면 총점을 높혀라)를 제시하고 프로그램이 어떤 알고리즘을 적용하는지는 상관하지 않되 목표를 달성하면 큰 점수를 부여하고(양수값의 점수) 목표를 달성하지 않으면 낮은 점수(이를테면 음수값의 점수)를 부여하여 **수백만번 같은 훈련**을 반복시킴

예시
- 지도학습 - 얼굴 감정 인식, 아이폰 첫 시리 세팅, 카메라 차량번호판 숫자 추출
- 비지도학습 - 연관상품 추천, 연관검색어, 유튜브 알고리즘, 휴대폰 얼굴인식(이미지 중에서 해당하는 부분을 뽑아내는 기술)
- 강화학습 - 주식투자

<br>

# 지도학습 - 선형 회귀
지도 학습: 라벨링된 기존 데이터를 학습시켜서 새로운 데이터가 왔을 때 결과 추론

### 회귀분석 Regression
함수
- x 하나에 y 하나가 대응 (인풋에 대한 결과값을 예측할 수 있어야 함 → 왜? 결과가 하나뿐일테니까)
- 입력과 출력이 있으면 함수

1차 함수
- 입력값(x)의 최고차수가 1차인 함수

### 선형회귀 (`Linear Regression`)
회귀선 - 데이터들과 오차의 합이 가장 적은 것이 적합한 것이 됨!           
기존 데이터들을 **가장 오차 없이 설명가능한 1차 함수의 식**을 놓은 후, 새로운 예측 데이터가 들어왔을 때 1차 함수를 바탕으로 예측하게 함 

*오차 = 비용 (cost) → 비용이 작은 함수가 목표! 
sigma n=1 to k (bn - f(an))^2


**Summary**
- h(x) = ax + b → 가설을 구하는 함수! (hypothesis)
    - **단서에 따라서 예측값이 정해지는** 함수
- cost 함수
    - 내가 세운 가설의 cost를 알게되는 함수
    - 예측 함수로부터 실데이터의 코스트

가설함수에 따라서 비용함수 값이 정해짐! 
h(x) = 3x+4 → 40 (cost 값)

cost 함수는 가설함수가 정해지면 **가설함수에 따른 비용 값**을 정해줌!!            
⇒ cost함수의 input값이 h(x)의 a, b ⇒ c(a, b)


### cost 함수
우리의 목표! : **cost 함수 값을 최소로 만드는 a와 b를 찾아야 함!**       

c(a, b) = c(w, b)       
h(x) = wx + b (w: weight, b: bias)      
c(w, b) = sigma k=1 to n (bk - h(ak))^2     
= sigma k=1 to n (bk - w*ak - b)^2

계산의 편의를 위해 h(x) = wx라 가정        
⇒ c(w) = sigma k=1 to n (bk - w*ak)^2 

즉, Linear Regerssion에서 h(x) = wx라 가정하면, **cost 함수는 w에 대한 2차 함수**임!      
2차 함수의 최대/최소 값을 구할 수 있듯이 **cost 함수의 최대/최소 값**을 구할 수 있다!         

최소 cost 값을 위해서 c'(w) = 0이 되는 w를 찾게 하면 됨     
c'(w) = 2(sigma k=1 to n ak^2)*w - 2(sigma k=1 to n ak*bk) = 0      
⇒ w = (sigma k=1 to n ak*bk) / (sigma k=1 to n ak^2)


# `Gradient Desent` - 경사하강법

### 여기서 의문!
토익 점수 영향 요소
- 토익공부 투입시간
- 나이
- 최종학력
- 유학기간
- 학원등록 여부
- 영단어 암기수

토익 점수에 영향끼치는 요소가 1가지만 있지는 않을 것. 분명 여러 개임         
→ 각 역향들이 모두 동일한 영향력을 갖는 것은 아님! 
- Single Linear Regression
    - 인자가 하나
    - h(x) = ax + b
- Multi Linear Regression
    - 인자가 여러개
    - h(x) = w1x1 + w2x2 + ... + wnxn + b    

h(x) = w_1x_1 + w_2x_2 + ... w_nx_n + b         
실제 가설함수는 변수가 다양할 것임 -> 각 변수의 **중요도(가중치: `weight`)**

cost(w1, w2, ..., wn, b)        
= sigma k=1 to n (bk - h(ak))^2
= sigma k=1 to n (bk - w1a1 - w2a2 - ... - wnan - b)^2

"편미분" - 미분하고자 하는 미지수 외에는 전부 상수 취급!      
i) w1에 대해 편미분 -> w1에 대한 1차식         
ii) w2에 대해 편미분 -> w2에 대한 1차식        
iii) w3에 대해 편미분 -> w3에 대한 1차식       
...     

=> 결국 식을 풀 수 없게 됨! 

### 경사하강법을 통해 해결해보자!
그래프의 기울기가 가파른 쪽으로 내려가다 보면 어느 순간 가장 낮은 값이 보이는데 그것을 최저값이라고 가정함.       
=> 어디서 시작하느냐에 따라 해당 구간에서 가장 낮은 값이 전 구간에서의 낮은 값임을 보장할 수 없음       
=> 경사하강법의 최대 단점!

### 해결?
그래프를 평평하게 펴야함! 어느 지점이 최저점인지 한번에 파악 할 수 있어야 함!       
현재 비용함수에 특정 공식을 곱하여 합성함수로 평평한 그래프를 만들면 됨! -> 경사하강법을 적용할 수 있는 극소값이 하나뿐이 그래프가 됨   

<br>

# Code 
실제 AI 알고리즘을 적용할 때는 w값을 식으로 한번에 구할 수 없기 때문에 경사하강법을 이용해 최적값을 찾아야 함       

c(w)의 최소값을 찾아감에 있어      
- c'(a)>0; a-=kc'(a) 반복
- c'(a)<0; a-=kc'(a) 반복 (c'(a)가 음수이므로 -임에 주의)
- c'(a)=0; 최소값을 찾았으므로 반복문 탈출
=> c(a): 함수 c(w)의 최소값

<br>

다음을 코드로 구현해보자면?
```python
# Single Linear Regression - cost 함수 c(w)의 최소값 구하기
# 단, h(x)=wx로 b는 무시

a = 랜덤값
for 충분히 반복: # 단, 유한 반복
    if c'(a) != 0:
        a -= k*c'(a) # 단 k는 임의의 실수
    else:
        for문 탈출

# w=a 일때 c(w)는 최소값 c(a)를 가짐
```

`min_w = min_w -k*c'(min_w)`        
다음 식에서 k를 `lr`로 용어 변경! (learning rate: 학습률)     

인공지능이 제대로 학습하기 위해서는 적당한 `lr`값을 정하는 것이 필수적임!!!
- 너무 큰 lr을 지정하면
    - 최소값을 영원히 못찾게 됨
    - cost가 오히려 올라갈 수 있음   
- 너무 작은 lr을 지정하면
    - 최소값까지 도달하는데 너무 오랜 시간이 걸릴 수도 있음
- 적당한 lr을 지정하면 cost가 계속 낮아지는 쪽으로 min_w 값이 조정되어 최소값에 도달하게 됨!!  

<br>

```python
import torch

"""학습할 데이터"""
x_train = torch.FloatTensor([[30],[60],[90]]) # 학습데이터의 x
y_train = torch.FloatTensor([[700],[750],[800]]) # 학습데이터의 y
# tensor: 3차원 데이터로 1,2 차원 데이터 연산도 가능 -> GPU 연산에 특화

# zeros(n): n차원의 tenser
W = torch.zeros(1) 
b = torch.zeros(1)

"""learning rate"""
lr = 0.0002

epochs = 400000

len_x = len(x_train)

for epoch in range(epochs):
  hypothesis = x_train * W + b
  cost = torch.mean((hypothesis -y_train)**2)

  gradient_w = torch.sum((W*x_train - y_train +b)*x_train)/ len_x # cost함수를 w에 대해서 함성함수 편미분 -> 편의상 2(상수) 없앰
  gradient_b = torch.sum((W*x_train - y_train +b))/len_x # cost함수를 b에 대해서 함성함수 편미분 / 학습 데이터의 개수만큼 나누어 평균으로 이동 

  W -= lr * gradient_w
  b -= lr * gradient_b
  
  if epoch % 10000 == 0: # for문 10000번마다 트랙킹
    print('Epoch {:4d}/{} W:{:.6f} b:{:.6f} Cost: {:.6f}'.format(epoch,epochs,W.item() ,b.item() , cost.item()))
```
