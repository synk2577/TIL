# Ch03 회귀 알고리즘과 모델 규제

## 02) 선형 회귀

### 선형 회귀

- k-최근접 이웃 알고리즘의 단점을 극복: 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측
- 특성이 하나인 경우, 어떤 직선을 학습하는 알고리즘
- 하나의 직선(`y=ax+b`)에서 a는 기울기, b는 y절편을 의미함  
  -> a = lr 객체의 `coef_`속성, b = lr 객체의 `intercept_` 속성
- 선형 휘귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 **계수** or **가중치**에 저장됨

```
from sklearn.linear_model import LinearRegression # 선형회귀
```

### 선형 회귀 장단점

- 장점: 훈련 세트를 범위의 데이터도 잘 예측
- 단점: 직선 그래프에서 x값이 아주 잘을 때 0보다 작은 결과값이 나올 수 있음  
  -> 따라서, 최적의 직선이 아니라 **최적의 곡선**을 찾자! = **다항 회귀**

### 모델 파라미터

머신러닝 모델이 특성에서 학습한 파라미터

### 다항 회귀

다항식을 사용한 선형 회귀

- 훈련세트에 제곱 항 추가
- 타깃값은 그대로 사용: 목 표하는 값은 어떤 그래프를 훈련든지 바꿀 필요 없음!!
- `LinearRegression` 클래스를 그대로 사용

### 다항 회귀 장단점

- 장점: 훈련 세트와 테스트 세트의 성능이 선형회귀보다 좋음
- 단점: 테스트 세트의 성능이 훈련세트 보다 조금 높아 **과소적합** 현상
