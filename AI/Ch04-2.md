# Ch04 다양한 분류 알고리즘

## 02) 확률적 경사 하강법

### 점진적 학습

앞서 훈련한 모델을 버리지 않고, 새로운 데이터에 대해서만 조금씩 더 훈련하는 학습

### 확률적 경사 하강법 (Stochastic Gradient Descent)

- 대표적인 점진적 학습 알고리즘
- 가장 가파른 길을 찾아 내려오지만 **조금씩 내려오는 것**이 중요
- 훈련세트에서 랜덤하게 **하나의 샘플**을 골라 경사하강법 수행
- **에포크(epoch)**: 훈련 세트를 한 번 모두 사용하는 과정
- **미니배치 경사 하강법**
  - 여러 개의 샘플을 사용해 경사하강법을 수행
- **배치 경사 하강법**
  - 극단적으로 전체 샘플을 사용해 경사하강법을 수행
  - 전체 데이터를 사용해 가장 안정적
  - 그만큼 컴퓨터 자원을 많이 사용
  - 데이터가 너무 많으면 한 번에 읽을 수 없음

### 손실함수 (Loss Function)

- 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준
- 손실 함수의 값이 작을수록 좋음
- 손실함수는 미분 가능해야 함 = 연속적
  - **로지스틱 회귀 모델의 확률** 0~1 -> **연속적**

### 로지스틱 손실 함수 (Rogistic Loss Fnction)

= 이진 크로스엔트로피 손실함수 (Binary Cross-entropy Loss Function)

- 양성 클래스(target = 1)
  - 손실: $ -log(예측확률) $
  - **확률이 1에서 멀어질수록** 아주 큰 양수
- 음성 클래스 (target = 0)
  - 손실: $ -log(1-예측확률) $
  - **확률이 0에서 멀어질수록** 아주 큰 양수

다중분류; 크로스엔트로피 손실함수 (Cross-entropy Loss Function) 사용

### SGDClassifier

- 확률적 경사하강법 분류 알고리즘
- 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춤
  
### 에포크와 과대/과소 적합

확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소/과대 적합 발생

- **조기종료(early stopping)**: 과대적합이 시작하기 전에 훈련을 멈추는 것
